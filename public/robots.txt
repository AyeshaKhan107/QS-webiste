# ==============================
# robots.txt for SEO & Marketing
# ==============================

# Applies to all search engines
User-agent: *

# Allow everything by default
Disallow:

# Block private or sensitive folders
Disallow: /admin/
Disallow: /login/
Disallow: /register/
Disallow: /api/

# Optional: block query parameters that may create duplicate content
Disallow: /*?utm_*
Disallow: /*?ref=*

# Crawl-delay (optional, to reduce server load)
Crawl-delay: 5

# Sitemap location (important for SEO)
Sitemap: https://yourdomain.com/sitemap.xml

# Optional: specify important pages to prioritize
# Note: search engines usually discover pages via sitemap,
# but you can hint via robots.txt
# Allow: /products/
# Allow: /blog/
